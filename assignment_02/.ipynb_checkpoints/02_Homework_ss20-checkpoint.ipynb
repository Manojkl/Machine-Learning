{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build a spam classifier using Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Description: \n",
    "- There are three datasets for training: TrainDataset1.csv, TrainDataset2.csv and TrainDataset3.txt. Each dataset contains short messages with the labels (ham or spam). \n",
    "- Analyse, clean and visualise these datasets.\n",
    "- Combine them into one big data set for the training\n",
    "- Use this dataset in order to build your own Naive Bayes classifier. (You can either use existing Naive Bayes from sklearn or build your own one)\n",
    "- Verify your Classifier using new messages (create your own messages or use the messages from the TestDataset.csv dataset).\n",
    "\n",
    "## Project Duration: 2 weeks\n",
    "## Project Deliverables:\n",
    "1. End of the first week do Data preprocessing: \n",
    "    - Load the dataset using pandas, \n",
    "    - Analysis it for this you will need to process the text, namely remove punctuation and stopwords, and then create a list of clean text words. (Research how to do this) \n",
    "    - Visualise the results\n",
    "    - Prepare the pre-processed data for the usage by Naive Bayes Classifier\n",
    "2. End of the second week:\n",
    "    - Train the classifier,\n",
    "    - Validate it, build confusion matrix, analyse its results\n",
    "    - Apply it to new test messages,\n",
    "    - Try to cheat the classifier by adding \"good words\" to the end of test message."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use the following link can be used as guidance for implementation:\n",
    "https://towardsdatascience.com/spam-filtering-using-naive-bayes-98a341224038"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matriculation number: 9038585\n",
    "### Name: Manoj Kolpe Lingappa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Trainingdataset1.csv', 'Trainingdataset2.csv', 'Trainingdataset3.csv']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/manoj/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:38: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>U were outbid by simonwatson5120 on the Shinco...</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Do you still have the grinder?</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No. Yes please. Been swimming?</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No de.am seeing in online shop so that i asked.</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Faith makes things possible,Hope makes things ...</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  type\n",
       "0  U were outbid by simonwatson5120 on the Shinco...  spam\n",
       "1                     Do you still have the grinder?   ham\n",
       "2                     No. Yes please. Been swimming?   ham\n",
       "3    No de.am seeing in online shop so that i asked.   ham\n",
       "4  Faith makes things possible,Hope makes things ...   ham"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import csv\n",
    "from glob import glob \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Read the csv files using pandas\n",
    "traindata1 = pd.read_csv(\"TrainDataset1.csv\")\n",
    "traindata1.to_csv(\"Trainingdataset1.csv\")\n",
    "traindata2 = pd.read_csv(\"TrainDataset2.csv\")\n",
    "traindata2.rename(columns = {\"v1\":\"type\",\"v2\":\"text\" }, inplace =True)\n",
    "traindata2.to_csv(\"Trainingdataset2.csv\")\n",
    "testdata = pd.read_csv(\"TestDataset.csv\")\n",
    "words = open(\"TrainDataset3.txt\").read().splitlines()\n",
    "label = []\n",
    "message = []\n",
    "for i in words:\n",
    "    processed_word = i.split(\"\\t\")\n",
    "    label.append(processed_word[0])\n",
    "    message.append(processed_word[1])\n",
    "with open(\"TrainDataset3.csv\", 'w', newline='') as f:\n",
    "    thewriter = csv.writer(f)\n",
    "    thewriter.writerow([\"type\",\"text\"])\n",
    "    for i in range(len(label)):\n",
    "        thewriter.writerow([label[i],message[i]])\n",
    "traindata3 = pd.read_csv(\"TrainDataset3.csv\")\n",
    "traindata3.to_csv(\"Trainingdataset3.csv\")\n",
    "stock_files = sorted(glob(\"Trainingdataset*.csv\"))\n",
    "print(stock_files)\n",
    "Trainingdata = pd.concat((pd.read_csv(file).assign(filename=file)\n",
    "                         for file in stock_files), ignore_index=True)\n",
    "Trainingdata = Trainingdata.drop(Trainingdata.columns[[0,1,2]],axis = 1)\n",
    "Trainingdata.to_csv(\"mergeddataset.csv\",index=False)\n",
    "Trainingdata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>U were outbid by simonwatson5120 on the Shinco...</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Do you still have the grinder?</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No. Yes please. Been swimming?</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No de.am seeing in online shop so that i asked.</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Faith makes things possible,Hope makes things ...</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  type\n",
       "0  U were outbid by simonwatson5120 on the Shinco...  spam\n",
       "1                     Do you still have the grinder?   ham\n",
       "2                     No. Yes please. Been swimming?   ham\n",
       "3    No de.am seeing in online shop so that i asked.   ham\n",
       "4  Faith makes things possible,Hope makes things ...   ham"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Describe Generate descriptive statistics that summarize the central tendency, dispersion and shape of a dataset's distribution\n",
    "trainingdataset = pd.read_csv(\"mergeddataset.csv\")\n",
    "trainingdataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>count</td>\n",
       "      <td>14488</td>\n",
       "      <td>14488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>unique</td>\n",
       "      <td>5622</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>top</td>\n",
       "      <td>Sorry, I'll call later</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>freq</td>\n",
       "      <td>86</td>\n",
       "      <td>12563</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          text   type\n",
       "count                    14488  14488\n",
       "unique                    5622      2\n",
       "top     Sorry, I'll call later    ham\n",
       "freq                        86  12563"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainingdataset.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"4\" halign=\"left\">text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>unique</th>\n",
       "      <th>top</th>\n",
       "      <th>freq</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>type</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>ham</td>\n",
       "      <td>12563</td>\n",
       "      <td>4784</td>\n",
       "      <td>Sorry, I'll call later</td>\n",
       "      <td>86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>spam</td>\n",
       "      <td>1925</td>\n",
       "      <td>838</td>\n",
       "      <td>HMV BONUS SPECIAL 500 pounds of genuine HMV vo...</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       text                                                               \n",
       "      count unique                                                top freq\n",
       "type                                                                      \n",
       "ham   12563   4784                             Sorry, I'll call later   86\n",
       "spam   1925    838  HMV BONUS SPECIAL 500 pounds of genuine HMV vo...    9"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainingdataset.groupby('type').describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>type</th>\n",
       "      <th>length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>U were outbid by simonwatson5120 on the Shinco...</td>\n",
       "      <td>spam</td>\n",
       "      <td>133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Do you still have the grinder?</td>\n",
       "      <td>ham</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No. Yes please. Been swimming?</td>\n",
       "      <td>ham</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No de.am seeing in online shop so that i asked.</td>\n",
       "      <td>ham</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Faith makes things possible,Hope makes things ...</td>\n",
       "      <td>ham</td>\n",
       "      <td>133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>Hey u still at the gym?</td>\n",
       "      <td>ham</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>Where is that one day training:-)</td>\n",
       "      <td>ham</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>Did I forget to tell you ? I want you , I need...</td>\n",
       "      <td>ham</td>\n",
       "      <td>142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>I dont thnk its a wrong calling between us</td>\n",
       "      <td>ham</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>December only! Had your mobile 11mths+? You ar...</td>\n",
       "      <td>spam</td>\n",
       "      <td>157</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  type  length\n",
       "0  U were outbid by simonwatson5120 on the Shinco...  spam     133\n",
       "1                     Do you still have the grinder?   ham      30\n",
       "2                     No. Yes please. Been swimming?   ham      30\n",
       "3    No de.am seeing in online shop so that i asked.   ham      47\n",
       "4  Faith makes things possible,Hope makes things ...   ham     133\n",
       "5                            Hey u still at the gym?   ham      23\n",
       "6                  Where is that one day training:-)   ham      33\n",
       "7  Did I forget to tell you ? I want you , I need...   ham     142\n",
       "8         I dont thnk its a wrong calling between us   ham      42\n",
       "9  December only! Had your mobile 11mths+? You ar...  spam     157"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create new column called \"length\" and apply the len function to the text column.\n",
    "trainingdataset[\"length\"] = trainingdataset[\"text\"].apply(len)\n",
    "trainingdataset.head(n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1.770e+02, 2.550e+02, 1.940e+03, 1.860e+03, 1.438e+03, 1.142e+03,\n",
       "        7.950e+02, 8.090e+02, 6.030e+02, 5.740e+02, 3.860e+02, 3.960e+02,\n",
       "        4.220e+02, 4.060e+02, 4.850e+02, 5.470e+02, 9.340e+02, 7.840e+02,\n",
       "        1.140e+02, 7.700e+01, 3.200e+01, 3.800e+01, 2.400e+01, 9.000e+00,\n",
       "        2.000e+01, 2.800e+01, 2.200e+01, 8.000e+00, 9.000e+00, 6.000e+00,\n",
       "        3.200e+01, 8.000e+00, 1.700e+01, 5.000e+00, 2.000e+00, 1.300e+01,\n",
       "        6.000e+00, 3.000e+00, 0.000e+00, 3.000e+00, 4.000e+00, 7.000e+00,\n",
       "        1.000e+00, 0.000e+00, 3.000e+00, 3.000e+00, 0.000e+00, 3.000e+00,\n",
       "        9.000e+00, 4.000e+00, 6.000e+00, 0.000e+00, 1.000e+00, 0.000e+00,\n",
       "        0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00,\n",
       "        0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00, 6.000e+00, 0.000e+00,\n",
       "        0.000e+00, 3.000e+00, 0.000e+00, 3.000e+00, 0.000e+00, 0.000e+00,\n",
       "        0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00,\n",
       "        0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00,\n",
       "        0.000e+00, 0.000e+00, 3.000e+00, 0.000e+00, 0.000e+00, 0.000e+00,\n",
       "        0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00,\n",
       "        0.000e+00, 0.000e+00, 0.000e+00, 3.000e+00]),\n",
       " array([  2.  ,  11.08,  20.16,  29.24,  38.32,  47.4 ,  56.48,  65.56,\n",
       "         74.64,  83.72,  92.8 , 101.88, 110.96, 120.04, 129.12, 138.2 ,\n",
       "        147.28, 156.36, 165.44, 174.52, 183.6 , 192.68, 201.76, 210.84,\n",
       "        219.92, 229.  , 238.08, 247.16, 256.24, 265.32, 274.4 , 283.48,\n",
       "        292.56, 301.64, 310.72, 319.8 , 328.88, 337.96, 347.04, 356.12,\n",
       "        365.2 , 374.28, 383.36, 392.44, 401.52, 410.6 , 419.68, 428.76,\n",
       "        437.84, 446.92, 456.  , 465.08, 474.16, 483.24, 492.32, 501.4 ,\n",
       "        510.48, 519.56, 528.64, 537.72, 546.8 , 555.88, 564.96, 574.04,\n",
       "        583.12, 592.2 , 601.28, 610.36, 619.44, 628.52, 637.6 , 646.68,\n",
       "        655.76, 664.84, 673.92, 683.  , 692.08, 701.16, 710.24, 719.32,\n",
       "        728.4 , 737.48, 746.56, 755.64, 764.72, 773.8 , 782.88, 791.96,\n",
       "        801.04, 810.12, 819.2 , 828.28, 837.36, 846.44, 855.52, 864.6 ,\n",
       "        873.68, 882.76, 891.84, 900.92, 910.  ]),\n",
       " <a list of 100 Patch objects>)"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualizing the distribution of length using histogram\n",
    "plt.hist(trainingdataset[\"length\"], 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    14488.000000\n",
       "mean        80.286582\n",
       "std         59.886021\n",
       "min          2.000000\n",
       "25%         36.000000\n",
       "50%         62.000000\n",
       "75%        122.000000\n",
       "max        910.000000\n",
       "Name: length, dtype: float64"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainingdataset.length.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"For me the love should start with attraction.i should feel that I need her every time around me.she should be the first thing which comes in my thoughts.I would start the day and end it with her.she should be there every time I dream.love will be then when my every breath has her name.my life should happen around her.my life will be named to her.I would cry for her.will give all my happiness and take all her sorrows.I will be ready to fight with anyone for her.I will be in love when I will be doing the craziest things for her.love will be when I don't have to proove anyone that my girl is the most beautiful lady on the whole planet.I will always be singing praises for her.love will be when I start up making chicken curry and end up makiing sambar.life will be the most beautiful then.will get every morning and thank god for the day because she is with me.I would like to say a lot..will tell later..\""
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Finding the text having length of 910 using iloc in pandas\n",
    "trainingdataset[trainingdataset[\"length\"]==910][\"text\"].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The data is in text format. The classification task need numeric feature vectors in the form of numbers. So we need to \n",
    "# convert our corpera to feature vectors. One simple method would be \"bag of approach\", where each unique word is\n",
    "# represented by one number.\n",
    "# Here raw messages(sequence of messages) to vector(sequence of number)\n",
    "# NLTK library is used for removing the stopwords. \n",
    "# We can remove the punctuation from a message by using the string() library\n",
    "\n",
    "def process(words):\n",
    "    nopunctu = [i for i in words if i not in string.punctuation]\n",
    "    nopunctu = ''.join(nopunctu)\n",
    "    return [k for k in nopunctu.split() if k.lower() not in stopwords.words(\"english\")]\n",
    "\n",
    "# new = 'hello dlkfjgdlfk?....'\n",
    "# process(new)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8767\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "                lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "                ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "                strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "                tokenizer=None, vocabulary=None)\n"
     ]
    }
   ],
   "source": [
    "# Now we have to tkoenize the text\n",
    "# There is numerous way we can normalize the words either by using steeming or part of speech\n",
    "# Sometimes it's nltk tool don't work well because of usage of shorthand or abbreviation. Ex. Hey was'up\n",
    "# Now we have each of tokens of message need to be converted into vector for learning using scikit leran in three step.\n",
    "# 1. Count how many times does a word occur in each message (Known as term frequency)\n",
    "# 2. Weigh the counts, so that frequent tokens get lower weight (inverse document frequency)\n",
    "# 3. Normalize the vectors to unit length, to abstract from the original text length (L2 norm)\n",
    "# Step 1\n",
    "# Number of dimension is equal to number of words in the corpus.\n",
    "# Scikit learn Countvectorizer covnvert the collection of text documents into matrix of token counts.\n",
    "# We have 2D vector space, 1 is the entire vocabulary other is actual document.\n",
    "# Because there is lot of messages we will get many zero count for presence of that word in the document.That's why Scikit learn will produce sparse matrix\n",
    "# bow = bag of words\n",
    "\n",
    "bow_transform = CountVectorizer().fit(trainingdataset[\"text\"])\n",
    "# bow_transform = CountVectorizer().fit(testdata[\"v2\"])\n",
    "print(len(bow_transform.vocabulary_))\n",
    "print(bow_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faith makes things possible,Hope makes things work,Love makes things beautiful,May you have all three this Christmas!Merry Christmas!\n"
     ]
    }
   ],
   "source": [
    "message1 = trainingdataset[\"text\"][4]\n",
    "print(message1)\n",
    "# message1 = \"hello, how, my name is billa valke ella\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 1043)\t1\n",
      "  (0, 1462)\t1\n",
      "  (0, 2064)\t2\n",
      "  (0, 3133)\t1\n",
      "  (0, 3808)\t1\n",
      "  (0, 3959)\t1\n",
      "  (0, 4792)\t1\n",
      "  (0, 4902)\t3\n",
      "  (0, 4983)\t1\n",
      "  (0, 5043)\t1\n",
      "  (0, 6032)\t1\n",
      "  (0, 7741)\t3\n",
      "  (0, 7751)\t1\n",
      "  (0, 7769)\t1\n",
      "  (0, 8578)\t1\n",
      "  (0, 8705)\t1\n",
      "(1, 8767)\n",
      "09061743386\n",
      "150ppermesssubscription\n",
      "wtlp\n"
     ]
    }
   ],
   "source": [
    "bow4 = bow_transform.transform([message1])\n",
    "print(bow4)\n",
    "print(bow4.shape)\n",
    "print(bow_transform.get_feature_names()[229])\n",
    "print(bow_transform.get_feature_names()[356])\n",
    "print(bow_transform.get_feature_names()[8627]) \n",
    "# Here first column is the position of the word and second column is the number of times word is repeated in message1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of sparse matrix. (14488, 8767)\n",
      "Number of non zero occurences. 192453\n"
     ]
    }
   ],
   "source": [
    "text_bow = bow_transform.transform(trainingdataset[\"text\"])\n",
    "print(\"Shape of sparse matrix.\", text_bow.shape)\n",
    "print(\"Number of non zero occurences.\", text_bow.nnz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sparsity:0\n"
     ]
    }
   ],
   "source": [
    "# In numerical analysis and scientific computing, a sparse matrix or sparse array is a matrix in which most of the\n",
    "# elements are zero. By contrast, if most of the elements are nonzero, then the matrix is considered dense. \n",
    "# The number of zero-valued elements divided by the total number of elements \n",
    "# (e.g., m × n for an m × n matrix) is called the sparsity of the matrix \n",
    "# (which is equal to 1 minus the density of the matrix). \n",
    "# Using those definitions, a matrix will be sparse when its sparsity is greater than 0.5.\n",
    "sparsity =(100.0 * text_bow.nnz/(text_bow.shape[0]*text_bow.shape[1]))\n",
    "print('sparsity:{}'.format(round(sparsity)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 8705)\t0.06169158534235719\n",
      "  (0, 8578)\t0.13798830045302007\n",
      "  (0, 7769)\t0.22299790296607458\n",
      "  (0, 7751)\t0.1048530443549801\n",
      "  (0, 7741)\t0.47641189493061215\n",
      "  (0, 6032)\t0.20410927406067444\n",
      "  (0, 5043)\t0.19852851751883147\n",
      "  (0, 4983)\t0.15857005340115776\n",
      "  (0, 4902)\t0.5344995843316507\n",
      "  (0, 4792)\t0.12144094688800114\n",
      "  (0, 3959)\t0.1337407500449679\n",
      "  (0, 3808)\t0.09148330923783803\n",
      "  (0, 3133)\t0.23531473770562764\n",
      "  (0, 2064)\t0.3950749597896546\n",
      "  (0, 1462)\t0.18066833260055135\n",
      "  (0, 1043)\t0.11159049575590801\n"
     ]
    }
   ],
   "source": [
    "# weighting and normalization is done with the help of TF-IDF(term freuency and Inverse document frequency)\n",
    "# Weight is a statistical measure of how important a word is for a documentin a corpus.\n",
    "# A word importance increases as number of times it appear in the document but is offsetted frequency number in the corpus.\n",
    "# Usually TF-IDF is used by search engines for ranking and scoring documents to show result for a query.\n",
    "# TF-IDF weight is made up of two terms: First is normalized term frequency(N-TF) i.e number of times a word occur\n",
    "# in a document to total number of terms in the document.Second is IDF, computed as the logarithm of the number \n",
    "# of the documents in the corpus divided by the number of documents where the specific term appears.\n",
    "# TF(t) = (Number of times term t appears in a document) / (Total number of terms in the document).\n",
    "# Normalization is done for TF because number of times word occur changes as length of message changes.\n",
    "# IDF(t) = log_e(Total number of documents / Number of documents with term t in it).\n",
    "# IDF will let us weight the importance of certain word, because some words may appera many times such as \"a\",\"be\"\n",
    "# which is of low importance hence need to be weighed less.\n",
    "# Ex.Consider a document containing 100 words wherein the word cat appears 3 times.\n",
    "# The term frequency (i.e., tf) for cat is then (3 / 100) = 0.03. Now, assume we have 10 million documents \n",
    "# and the word cat appears in one thousand of these. Then, the inverse document frequency (i.e., idf) is \n",
    "# calculated as log(10,000,000 / 1,000) = 4.\n",
    "# Thus, the Tf-idf weight is the product of these quantities: 0.03 * 4 = 0.12.\n",
    "tfid_transformer = TfidfTransformer().fit(text_bow)\n",
    "tfid_message1 = tfid_transformer.transform(bow4)\n",
    "print(tfid_message1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.803492696598066\n",
      "8.97170710738662\n"
     ]
    }
   ],
   "source": [
    "# We can compute the tfidf value for two words \"hi\" and \"hint\"\n",
    "print(tfid_transformer.idf_[bow_transform.vocabulary_['hi']])\n",
    "print(tfid_transformer.idf_[bow_transform.vocabulary_['hint']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14488, 8767)\n"
     ]
    }
   ],
   "source": [
    "text_tfidf = tfid_transformer.transform(text_bow)\n",
    "print(text_tfidf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have fitted the type to multinomial naive bayes classifier\n",
    "detector = MultinomialNB().fit(text_tfidf,trainingdataset[\"type\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: ham\n",
      "Actual: ham\n"
     ]
    }
   ],
   "source": [
    "print(\"Predicted:\", detector.predict(tfid_message1)[0])\n",
    "print(\"Actual:\", traindata1.type[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['spam' 'ham' 'ham' ... 'ham' 'ham' 'ham']\n"
     ]
    }
   ],
   "source": [
    "# Evaluation of model\n",
    "# We will do all the prediction\n",
    "\n",
    "all_predict = detector.predict(text_tfidf)\n",
    "print(all_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ham       0.99      1.00      0.99     12563\n",
      "        spam       1.00      0.93      0.97      1925\n",
      "\n",
      "    accuracy                           0.99     14488\n",
      "   macro avg       0.99      0.97      0.98     14488\n",
      "weighted avg       0.99      0.99      0.99     14488\n",
      "\n",
      "[[12562     1]\n",
      " [  128  1797]]\n"
     ]
    }
   ],
   "source": [
    "# We can use the scikit learn tool to find  precision, recall, f1-score, \n",
    "# and a column for support (meaning how many cases supported that classification).\n",
    "# Suppose a computer program for recognizing dogs in photographs identifies 8 dogs in a picture \n",
    "# containing 12 dogs and some cats. Of the 8 identified as dogs, 5 actually are dogs (true positives), \n",
    "# while the rest are cats (false positives). The program's precision is 5/8 while its recall is 5/12.\n",
    "# The F1 score is the harmonic mean of the precision and recall, where an F1 score reaches its best value at 1 (perfect precision and recall)\n",
    "# The evaluation model is depend on the task at hand. For ex. cost of predicting spam as ham is less compared to\n",
    "# predicting ham as spam\n",
    "print(classification_report(trainingdataset[\"type\"],all_predict))\n",
    "print(confusion_matrix(trainingdataset[\"type\"],all_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can test our trained model on the testing data\n",
    "pipeline = Pipeline([('bow', CountVectorizer()),('tfidf',TfidfTransformer()),('classifier',MultinomialNB())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "   ( 'bow',CountVectorizer()),\n",
    "    ('tfidf',TfidfTransformer()),\n",
    "    ('classifier',MultinomialNB()),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'ham': 973, 'spam': 142})\n"
     ]
    }
   ],
   "source": [
    "# pipeline.fit(testdata['text'],testdata['type']\n",
    "pipeline.fit(trainingdataset['text'],trainingdataset['type'])\n",
    "predict = pipeline.predict(testdata['v2'])\n",
    "# print(classification_report(predict,))\n",
    "print(Counter(predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10141 4347 10141 4347\n"
     ]
    }
   ],
   "source": [
    "# Splitting the dataset for training and testing in 70/30 ratio\n",
    "text_train,text_test,type_train,type_test = train_test_split(trainingdataset['text'],trainingdataset['type'],test_size=0.3)\n",
    "print(len(text_train),len(text_test),len(type_train),len(type_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('bow',\n",
       "                 CountVectorizer(analyzer='word', binary=False,\n",
       "                                 decode_error='strict',\n",
       "                                 dtype=<class 'numpy.int64'>, encoding='utf-8',\n",
       "                                 input='content', lowercase=True, max_df=1.0,\n",
       "                                 max_features=None, min_df=1,\n",
       "                                 ngram_range=(1, 1), preprocessor=None,\n",
       "                                 stop_words=None, strip_accents=None,\n",
       "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                 tokenizer=None, vocabulary=None)),\n",
       "                ('tfidf',\n",
       "                 TfidfTransformer(norm='l2', smooth_idf=True,\n",
       "                                  sublinear_tf=False, use_idf=True)),\n",
       "                ('classifier',\n",
       "                 MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.fit(text_train,type_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the model\n",
    "prediction = pipeline.predict(text_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ham       1.00      0.99      0.99      3847\n",
      "        spam       0.91      1.00      0.95       500\n",
      "\n",
      "    accuracy                           0.99      4347\n",
      "   macro avg       0.95      0.99      0.97      4347\n",
      "weighted avg       0.99      0.99      0.99      4347\n",
      "\n",
      "[[3797   50]\n",
      " [   0  500]]\n"
     ]
    }
   ],
   "source": [
    "# Classification report\n",
    "# confusion matrix\n",
    "print(classification_report(prediction, type_test))\n",
    "print(confusion_matrix(prediction, type_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
